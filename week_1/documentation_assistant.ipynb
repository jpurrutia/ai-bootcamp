{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c438df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "import json\n",
    "from minsearch import Index, VectorSearch\n",
    "import pickle \n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e27e114d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from typing import Iterable, Callable\n",
    "import zipfile\n",
    "import traceback\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RawRepositoryFile:\n",
    "    filename: str\n",
    "    content: str\n",
    "\n",
    "\n",
    "class GithubRepositoryDataReader:\n",
    "    \"\"\"\n",
    "    Downloads and parses markdown and code files from a GitHub repository.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                repo_owner: str,\n",
    "                repo_name: str,\n",
    "                allowed_extensions: Iterable[str] | None = None,\n",
    "                filename_filter: Callable[[str], bool] | None = None\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initialize the GitHub repository data reader.\n",
    "        \n",
    "        Args:\n",
    "            repo_owner: The owner/organization of the GitHub repository\n",
    "            repo_name: The name of the GitHub repository\n",
    "            allowed_extensions: Optional set of file extensions to include\n",
    "                    (e.g., {\"md\", \"py\"}). If not provided, all file types are included\n",
    "            filename_filter: Optional callable to filter files by their path\n",
    "        \"\"\"\n",
    "        prefix = \"https://codeload.github.com\"\n",
    "        self.url = (\n",
    "            f\"{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main\"\n",
    "        )\n",
    "\n",
    "        if allowed_extensions is not None:\n",
    "            self.allowed_extensions = {ext.lower() for ext in allowed_extensions}\n",
    "\n",
    "        if filename_filter is None:\n",
    "            self.filename_filter = lambda filepath: True\n",
    "        else:\n",
    "            self.filename_filter = filename_filter\n",
    "\n",
    "    def read(self) -> list[RawRepositoryFile]:\n",
    "        \"\"\"\n",
    "        Download and extract files from the GitHub repository.\n",
    "        \n",
    "        Returns:\n",
    "            List of RawRepositoryFile objects for each processed file\n",
    "            \n",
    "        Raises:\n",
    "            Exception: If the repository download fails\n",
    "        \"\"\"\n",
    "        resp = requests.get(self.url)\n",
    "        if resp.status_code != 200:\n",
    "            raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "        zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "        repository_data = self._extract_files(zf)\n",
    "        zf.close()\n",
    "\n",
    "        return repository_data\n",
    "\n",
    "    def _extract_files(self, zf: zipfile.ZipFile) -> list[RawRepositoryFile]:\n",
    "        \"\"\"\n",
    "        Extract and process files from the zip archive.\n",
    "        \n",
    "        Args:\n",
    "            zf: ZipFile object containing the repository data\n",
    "\n",
    "        Returns:\n",
    "            List of RawRepositoryFile objects for each processed file\n",
    "        \"\"\"\n",
    "        data = []\n",
    "\n",
    "        for file_info in zf.infolist():\n",
    "            filepath = self._normalize_filepath(file_info.filename)\n",
    "\n",
    "            if self._should_skip_file(filepath):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with zf.open(file_info) as f_in:\n",
    "                    content = f_in.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "                    if content is not None:\n",
    "                        content = content.strip()\n",
    "\n",
    "                    file = RawRepositoryFile(\n",
    "                        filename=filepath,\n",
    "                        content=content\n",
    "                    )\n",
    "                    data.append(file)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_info.filename}: {e}\")\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _should_skip_file(self, filepath: str) -> bool:\n",
    "        \"\"\"\n",
    "        Determine whether a file should be skipped during processing.\n",
    "        \n",
    "        Args:\n",
    "            filepath: The file path to check\n",
    "            \n",
    "        Returns:\n",
    "            True if the file should be skipped, False otherwise\n",
    "        \"\"\"\n",
    "        filepath = filepath.lower()\n",
    "\n",
    "        # directory\n",
    "        if filepath.endswith(\"/\"):\n",
    "            return True\n",
    "\n",
    "        # hidden file\n",
    "        filename = filepath.split(\"/\")[-1]\n",
    "        if filename.startswith(\".\"):\n",
    "            return True\n",
    "\n",
    "        if self.allowed_extensions:\n",
    "            ext = self._get_extension(filepath)\n",
    "            if ext not in self.allowed_extensions:\n",
    "                return True\n",
    "\n",
    "        if not self.filename_filter(filepath):\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _get_extension(self, filepath: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract the file extension from a filepath.\n",
    "        \n",
    "        Args:\n",
    "            filepath: The file path to extract extension from\n",
    "            \n",
    "        Returns:\n",
    "            The file extension (without dot) or empty string if no extension\n",
    "        \"\"\"\n",
    "        filename = filepath.lower().split(\"/\")[-1]\n",
    "        if \".\" in filename:\n",
    "            return filename.rsplit(\".\", maxsplit=1)[-1]\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    def _normalize_filepath(self, filepath: str) -> str:\n",
    "        \"\"\"\n",
    "        Removes the top-level directory from the file path inside the zip archive.\n",
    "        'repo-main/path/to/file.py' -> 'path/to/file.py'\n",
    "        \n",
    "        Args:\n",
    "            filepath: The original filepath from the zip archive\n",
    "            \n",
    "        Returns:\n",
    "            The normalized filepath with top-level directory removed\n",
    "        \"\"\"\n",
    "        parts = filepath.split(\"/\", maxsplit=1)\n",
    "        if len(parts) > 1:\n",
    "            return parts[1]\n",
    "        else:\n",
    "            return parts[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5833c1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_github_data():\n",
    "    allowed_extensions = {\"md\", \"mdx\"}\n",
    "\n",
    "    repo_owner = 'evidentlyai'\n",
    "    repo_name = 'docs'\n",
    "\n",
    "    reader = GithubRepositoryDataReader(\n",
    "        repo_owner,\n",
    "        repo_name,\n",
    "        allowed_extensions=allowed_extensions\n",
    "    )\n",
    "    \n",
    "    return reader.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d8c45120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 95 files\n"
     ]
    }
   ],
   "source": [
    "github_data = read_github_data()\n",
    "print(f\"Downloaded {len(github_data)} files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d8b31193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'---\\ntitle: \"Evidently and GitHub actions\"\\ndescription: \"Testing LLM outputs as part of the CI/CD flow.\"\\n---\\n\\nYou can use Evidently together with GitHub Actions to automatically test the outputs of your LLM agent or application - as part of every code push or pull request.\\n\\n## How the integration work:\\n\\n- You define a test dataset of inputs (e.g. test prompts with or without reference answers). You can store it as a file, or save the dataset at Evidently Cloud callable by Dataset ID.\\n- Run your LLM system or agent against those inputs inside CI.\\n- Evidently automatically evaluates the outputs using the user-specified config (which defines the Evidently descriptors, tests and Report composition), including methods like:\\n  - LLM judges (e.g., tone, helpfulness, correctness)\\n  - Custom Python functions\\n  - Dataset-level metrics like classification quality\\n- If any test fails, the CI job fails.\\n- You get a detailed test report with pass/fail status and metrics.\\n\\n![](/images/examples/github_actions.gif)\\n\\nResults are stored locally or pushed to Evidently Cloud for deeper review and tracking.\\n\\nThe final result is CI-native testing for your LLM behavior - so you can safely tweak prompts, models, or logic without breaking things silently.\\n\\n## Code example and tutorial\\n\\n👉 Check the full tutorial and example repo: https://github.com/evidentlyai/evidently-ci-example\\n\\nAction is also available on GitHub Marketplace: https://github.com/marketplace/actions/run-evidently-report'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "github_data[40].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9c66eb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import frontmatter\n",
    "\n",
    "def parse_data(data_raw: List[RawRepositoryFile]) -> List[Dict[str, Any]]:\n",
    "\n",
    "    data_parsed = []\n",
    "    for f in data_raw:\n",
    "        post = frontmatter.loads(f.content)\n",
    "        data = post.to_dict()\n",
    "        data['filename'] = f.filename\n",
    "        data_parsed.append(data)\n",
    "\n",
    "    return data_parsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25e7b9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Document chunking utilities for splitting large documents into smaller, overlapping pieces.\n",
    "\n",
    "This module provides functionality to break down documents into chunks using a sliding\n",
    "window approach, which is useful for processing large texts in smaller, manageable pieces\n",
    "while maintaining context through overlapping content.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Any, Dict, Iterable, List\n",
    "\n",
    "\n",
    "def sliding_window(\n",
    "        seq: Iterable[Any],\n",
    "        size: int,\n",
    "        step: int\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create overlapping chunks from a sequence using a sliding window approach.\n",
    "\n",
    "    Args:\n",
    "        seq: The input sequence (string or list) to be chunked.\n",
    "        size (int): The size of each chunk/window.\n",
    "        step (int): The step size between consecutive windows.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each containing:\n",
    "            - 'start': The starting position of the chunk in the original sequence\n",
    "            - 'content': The chunk content\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If size or step are not positive integers.\n",
    "\n",
    "    Example:\n",
    "        >>> sliding_window(\"hello world\", size=5, step=3)\n",
    "        [{'start': 0, 'content': 'hello'}, {'start': 3, 'content': 'lo wo'}]\n",
    "    \"\"\"\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        batch = seq[i:i+size]\n",
    "        result.append({'start': i, 'content': batch})\n",
    "        if i + size > n:\n",
    "            break\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def chunk_documents(\n",
    "        documents: Iterable[Dict[str, str]],\n",
    "        size: int = 2000,\n",
    "        step: int = 1000,\n",
    "        content_field_name: str = 'content'\n",
    ") -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Split a collection of documents into smaller chunks using sliding windows.\n",
    "\n",
    "    Takes documents and breaks their content into overlapping chunks while preserving\n",
    "    all other document metadata (filename, etc.) in each chunk.\n",
    "\n",
    "    Args:\n",
    "        documents: An iterable of document dictionaries. Each document must have a content field.\n",
    "        size (int, optional): The maximum size of each chunk. Defaults to 2000.\n",
    "        step (int, optional): The step size between chunks. Defaults to 1000.\n",
    "        content_field_name (str, optional): The name of the field containing document content.\n",
    "                                          Defaults to 'content'.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of chunk dictionaries. Each chunk contains:\n",
    "            - All original document fields except the content field\n",
    "            - 'start': Starting position of the chunk in original content\n",
    "            - 'content': The chunk content\n",
    "\n",
    "    Example:\n",
    "        >>> documents = [{'content': 'long text...', 'filename': 'doc.txt'}]\n",
    "        >>> chunks = chunk_documents(documents, size=100, step=50)\n",
    "        >>> # Or with custom content field:\n",
    "        >>> documents = [{'text': 'long text...', 'filename': 'doc.txt'}]\n",
    "        >>> chunks = chunk_documents(documents, content_field_name='text')\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for doc in documents:\n",
    "        doc_copy = doc.copy()\n",
    "        doc_content = doc_copy.pop(content_field_name)\n",
    "        chunks = sliding_window(doc_content, size=size, step=step)\n",
    "        for chunk in chunks:\n",
    "            chunk.update(doc_copy)\n",
    "        results.extend(chunks)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7d5f0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Document indexing utilities for creating searchable indexes from document collections.\n",
    "\n",
    "This module provides functionality to index documents using minsearch, with optional\n",
    "chunking support for handling large documents.\n",
    "\"\"\"\n",
    "\n",
    "from minsearch import Index\n",
    "\n",
    "#from common.chunking import chunk_documents\n",
    "\n",
    "\n",
    "def index_documents(documents, chunk: bool = False, chunking_params=None) -> Index:\n",
    "    \"\"\"\n",
    "    Create a searchable index from a collection of documents.\n",
    "\n",
    "    Args:\n",
    "        documents: A collection of document dictionaries, each containing at least\n",
    "                  'content' and 'filename' fields.\n",
    "        chunk (bool, optional): Whether to chunk documents before indexing.\n",
    "                               Defaults to False.\n",
    "        chunking_params (dict, optional): Parameters for document chunking.\n",
    "                                        Defaults to {'size': 2000, 'step': 1000}.\n",
    "                                        Only used when chunk=True.\n",
    "\n",
    "    Returns:\n",
    "        Index: A fitted minsearch Index object ready for searching.\n",
    "\n",
    "    Example:\n",
    "        >>> docs = [{'content': 'Hello world', 'filename': 'doc1.txt'}]\n",
    "        >>> index = index_documents(docs)\n",
    "        >>> results = index.search('hello')\n",
    "    \"\"\"\n",
    "    if chunk:\n",
    "        if chunking_params is None:\n",
    "            chunking_params = {'size': 2000, 'step': 1000}\n",
    "        documents = chunk_documents(documents, **chunking_params)\n",
    "\n",
    "    index = Index(\n",
    "        text_fields=[\"content\", \"filename\"],\n",
    "    )\n",
    "\n",
    "    index.fit(documents)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bcb421b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from typing import Iterable, Callable\n",
    "import zipfile\n",
    "import traceback\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RawRepositoryFile:\n",
    "    filename: str\n",
    "    content: str\n",
    "\n",
    "\n",
    "class GithubRepositoryDataReader:\n",
    "    \"\"\"\n",
    "    Downloads and parses markdown and code files from a GitHub repository.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                repo_owner: str,\n",
    "                repo_name: str,\n",
    "                allowed_extensions: Iterable[str] | None = None,\n",
    "                filename_filter: Callable[[str], bool] | None = None\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initialize the GitHub repository data reader.\n",
    "        \n",
    "        Args:\n",
    "            repo_owner: The owner/organization of the GitHub repository\n",
    "            repo_name: The name of the GitHub repository\n",
    "            allowed_extensions: Optional set of file extensions to include\n",
    "                    (e.g., {\"md\", \"py\"}). If not provided, all file types are included\n",
    "            filename_filter: Optional callable to filter files by their path\n",
    "        \"\"\"\n",
    "        prefix = \"https://codeload.github.com\"\n",
    "        self.url = (\n",
    "            f\"{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main\"\n",
    "        )\n",
    "\n",
    "        if allowed_extensions is not None:\n",
    "            self.allowed_extensions = {ext.lower() for ext in allowed_extensions}\n",
    "\n",
    "        if filename_filter is None:\n",
    "            self.filename_filter = lambda filepath: True\n",
    "        else:\n",
    "            self.filename_filter = filename_filter\n",
    "\n",
    "    def read(self) -> list[RawRepositoryFile]:\n",
    "        \"\"\"\n",
    "        Download and extract files from the GitHub repository.\n",
    "        \n",
    "        Returns:\n",
    "            List of RawRepositoryFile objects for each processed file\n",
    "            \n",
    "        Raises:\n",
    "            Exception: If the repository download fails\n",
    "        \"\"\"\n",
    "        resp = requests.get(self.url)\n",
    "        if resp.status_code != 200:\n",
    "            raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "        zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "        repository_data = self._extract_files(zf)\n",
    "        zf.close()\n",
    "\n",
    "        return repository_data\n",
    "\n",
    "    def _extract_files(self, zf: zipfile.ZipFile) -> list[RawRepositoryFile]:\n",
    "        \"\"\"\n",
    "        Extract and process files from the zip archive.\n",
    "        \n",
    "        Args:\n",
    "            zf: ZipFile object containing the repository data\n",
    "\n",
    "        Returns:\n",
    "            List of RawRepositoryFile objects for each processed file\n",
    "        \"\"\"\n",
    "        data = []\n",
    "\n",
    "        for file_info in zf.infolist():\n",
    "            filepath = self._normalize_filepath(file_info.filename)\n",
    "\n",
    "            if self._should_skip_file(filepath):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with zf.open(file_info) as f_in:\n",
    "                    content = f_in.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "                    if content is not None:\n",
    "                        content = content.strip()\n",
    "\n",
    "                    file = RawRepositoryFile(\n",
    "                        filename=filepath,\n",
    "                        content=content\n",
    "                    )\n",
    "                    data.append(file)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_info.filename}: {e}\")\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _should_skip_file(self, filepath: str) -> bool:\n",
    "        \"\"\"\n",
    "        Determine whether a file should be skipped during processing.\n",
    "        \n",
    "        Args:\n",
    "            filepath: The file path to check\n",
    "            \n",
    "        Returns:\n",
    "            True if the file should be skipped, False otherwise\n",
    "        \"\"\"\n",
    "        filepath = filepath.lower()\n",
    "\n",
    "        # directory\n",
    "        if filepath.endswith(\"/\"):\n",
    "            return True\n",
    "\n",
    "        # hidden file\n",
    "        filename = filepath.split(\"/\")[-1]\n",
    "        if filename.startswith(\".\"):\n",
    "            return True\n",
    "\n",
    "        if self.allowed_extensions:\n",
    "            ext = self._get_extension(filepath)\n",
    "            if ext not in self.allowed_extensions:\n",
    "                return True\n",
    "\n",
    "        if not self.filename_filter(filepath):\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _get_extension(self, filepath: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract the file extension from a filepath.\n",
    "        \n",
    "        Args:\n",
    "            filepath: The file path to extract extension from\n",
    "            \n",
    "        Returns:\n",
    "            The file extension (without dot) or empty string if no extension\n",
    "        \"\"\"\n",
    "        filename = filepath.lower().split(\"/\")[-1]\n",
    "        if \".\" in filename:\n",
    "            return filename.rsplit(\".\", maxsplit=1)[-1]\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    def _normalize_filepath(self, filepath: str) -> str:\n",
    "        \"\"\"\n",
    "        Removes the top-level directory from the file path inside the zip archive.\n",
    "        'repo-main/path/to/file.py' -> 'path/to/file.py'\n",
    "        \n",
    "        Args:\n",
    "            filepath: The original filepath from the zip archive\n",
    "            \n",
    "        Returns:\n",
    "            The normalized filepath with top-level directory removed\n",
    "        \"\"\"\n",
    "        parts = filepath.split(\"/\", maxsplit=1)\n",
    "        if len(parts) > 1:\n",
    "            return parts[1]\n",
    "        else:\n",
    "            return parts[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6b15fb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data = parse_data(github_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f834ac06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You can view or export Reports in multiple formats.\\n\\n**Pre-requisites**:\\n\\n* You know how to [generate Reports](/docs/library/report).\\n\\n## Log to Workspace\\n\\nYou can save the computed Report in Evidently Cloud or your local workspace.\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=False)\\n```\\n\\n<Info>\\n  **Uploading evals**. Check Quickstart examples [for ML](/quickstart_ml) or [for LLM](/quickstart_llm) for a full workflow.\\n</Info>\\n\\n## View in Jupyter notebook\\n\\nYou can directly render the visual summary of evaluation results in interactive Python environments like Jupyter notebook or Colab.\\n\\nAfter running the Report, simply call the resulting Python object:\\n\\n```python\\nmy_report\\n```\\n\\nThis will render the HTML object directly in the notebook cell.\\n\\n## HTML\\n\\nYou can also save this interactive visual Report as an HTML file to open in a browser:\\n\\n```python\\nmy_report.save_html(“file.html”)\\n```\\n\\nThis option is useful for sharing Reports with others or if you\\'re working in a Python environment that doesn’t display interactive visuals.\\n\\n## JSON\\n\\nYou can get the results of the calculation as a JSON. It is useful for storing and exporting results elsewhere.\\n\\nTo view the JSON in Python:\\n\\n```python\\nmy_report.json()\\n```\\n\\nTo save the JSON as a separate file:\\n\\n```python\\nmy_report.save_json(\"file.json\")\\n```\\n\\n## Python dictionary\\n\\nYou can get the output as a Python dictionary. This format is convenient for automated evaluations in data or ML pipelines, allowing you to transform the output or extract specific values.\\n\\nTo get the dictionary:\\n\\n```python\\nmy_report.dict()\\n```'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_data[10]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "131332d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = index_documents(\n",
    "        parsed_data,\n",
    "        chunk=True,\n",
    "        chunking_params={\"size\": 2000, \"step\": 1000},\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "03fd9f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 1000,\n",
       "  'content': 'mply pass the selected Preset to the Report and run it over your data. If nothing else is specified, the Report will run with the default parameters for all columns in the dataset. \\n\\n**Single dataset**. To generate the Data Summary Report for a single dataset:\\n\\n```python\\nreport = Report([\\n    DataSummaryPreset()\\n])\\n\\nmy_eval = report.run(eval_data_1, None)\\nmy_eval\\n#my_eval.json\\n```\\n\\nAfter you `run` the Report, the resulting `my_eval` will contains the computed values for each metric, along with associated metadata and visualizations. (We sometimes refer to this computation result as a `snapshot`).\\n\\n<Note>\\nYou can render the results in Python, export as HTML, JSON or Python dictionary or upload to the Evidently platform. Check more in [output formats](/docs/library/output_formats).\\n</Note>\\n\\n**Two datasets**. To generate reports like Data Drift that needs two datasets, pass the second one as a reference when you `run` it:\\n\\n```python\\nreport = Report([\\n    DataDriftPreset()\\n])\\n\\nmy_eval = report.run(eval_data_1, eval_data_2)\\nmy_eval\\n#my_eval.json\\n```\\n\\nIn this case the first `eval_data_1` is the current data you evaluate, the second `eval_data_2` is the reference dataset you consider as a baseline for drift detection. You can also pass it explicitly:\\n\\n```\\nmy_eval = report.run(current_data=eval_data_1, reference_data=eval_data_2)\\n```\\n\\n**Combine Presets**. You can also include multiple Presets in the same Report. List them one by one.\\n\\n```python\\nreport = Report([\\n    DataDriftPreset(), \\n    DataSummaryPreset()\\n])\\n\\nmy_eval = report.run(eval_data_1, eval_data_2)\\nmy_eval\\n#my_eval.json\\n```\\n\\n**Limit columns**. You can limit the columns to which the Preset is applied.\\n\\n```python\\nreport = Report([\\n    DataDriftPreset(column=[\"target\", \"prediction\"])\\n])\\n\\nmy_eval = report.run(eval_data_1, eval_data_2)\\nmy_eval\\n#my_eval.json\\n```\\n\\n## Custom Report\\n\\n<Tip>\\n  **Available Metrics and parameters**. Check available evals in the [Reference table](/metrics/all_metrics).\\n</Tip>\\n\\n**Choose Metrics',\n",
       "  'title': 'Report',\n",
       "  'description': 'How to generate Report.',\n",
       "  'filename': 'docs/library/report.mdx'},\n",
       " {'start': 0,\n",
       "  'content': 'Reports perform evaluations on the Dataset level and/or summarize results of the row-level evaluations. For a general introduction, check [Core Concepts](/docs/library/overview).\\n\\n**Pre-requisites**:\\n\\n* You [installed Evidently](/docs/setup/installation).\\n\\n* You created a Dataset with the [Data Definition](/docs/library/data_definition).\\n\\n* (Optional) for text data, you added Descriptors.\\n\\n<Note>\\n  For a quick end-to-end example of generating Reports, check the Quickstart [for ML](/quickstart_ml) or [LLM](/quickstart_llm).\\n</Note>\\n\\n## Imports\\n\\nImport the Metrics and Presets you plan to use.\\n\\n```python\\nfrom evidently import Report\\nfrom evidently.metrics import *\\nfrom evidently.presets import *\\n```\\n\\nYou can use Metric Presets, which are pre-built Reports that work out of the box, or create a custom Report selecting Metrics one by one.\\n\\n## Presets\\n\\n<Tip>\\n  **Available Presets**. Check available evals in the [Reference table](/metrics/all_metrics).\\n</Tip>\\n\\nTo generate a template Report, simply pass the selected Preset to the Report and run it over your data. If nothing else is specified, the Report will run with the default parameters for all columns in the dataset. \\n\\n**Single dataset**. To generate the Data Summary Report for a single dataset:\\n\\n```python\\nreport = Report([\\n    DataSummaryPreset()\\n])\\n\\nmy_eval = report.run(eval_data_1, None)\\nmy_eval\\n#my_eval.json\\n```\\n\\nAfter you `run` the Report, the resulting `my_eval` will contains the computed values for each metric, along with associated metadata and visualizations. (We sometimes refer to this computation result as a `snapshot`).\\n\\n<Note>\\nYou can render the results in Python, export as HTML, JSON or Python dictionary or upload to the Evidently platform. Check more in [output formats](/docs/library/output_formats).\\n</Note>\\n\\n**Two datasets**. To generate reports like Data Drift that needs two datasets, pass the second one as a reference when you `run` it:\\n\\n```python\\nreport = Report([\\n    DataDriftPreset()\\n])\\n\\nmy_eval = re',\n",
       "  'title': 'Report',\n",
       "  'description': 'How to generate Report.',\n",
       "  'filename': 'docs/library/report.mdx'},\n",
       " {'start': 2000,\n",
       "  'content': 'port.run(eval_data_1, eval_data_2)\\nmy_eval\\n#my_eval.json\\n```\\n\\nIn this case the first `eval_data_1` is the current data you evaluate, the second `eval_data_2` is the reference dataset you consider as a baseline for drift detection. You can also pass it explicitly:\\n\\n```\\nmy_eval = report.run(current_data=eval_data_1, reference_data=eval_data_2)\\n```\\n\\n**Combine Presets**. You can also include multiple Presets in the same Report. List them one by one.\\n\\n```python\\nreport = Report([\\n    DataDriftPreset(), \\n    DataSummaryPreset()\\n])\\n\\nmy_eval = report.run(eval_data_1, eval_data_2)\\nmy_eval\\n#my_eval.json\\n```\\n\\n**Limit columns**. You can limit the columns to which the Preset is applied.\\n\\n```python\\nreport = Report([\\n    DataDriftPreset(column=[\"target\", \"prediction\"])\\n])\\n\\nmy_eval = report.run(eval_data_1, eval_data_2)\\nmy_eval\\n#my_eval.json\\n```\\n\\n## Custom Report\\n\\n<Tip>\\n  **Available Metrics and parameters**. Check available evals in the [Reference table](/metrics/all_metrics).\\n</Tip>\\n\\n**Choose Metrics**. To create a custom Report, simply list the Metics one by one. You can combine both dataset-level and column-level Metrics, and combine Presets and Metrics in one Report. When you use a column-level Metric, you must specify the column it refers to.\\n\\n```python\\nreport = Report([\\n    ColumnCount(), \\n    ValueStats(column=\"target\")\\n])\\n\\nmy_eval = report.run(eval_data_1, None)\\nmy_eval\\n#my_eval.json\\n```\\n\\n<Note>\\n  **Generating multiple column-level Metrics**: You can use a helper function to easily generate multiple column-level Metrics for a list of columns. See the page on [Metric Generator](/docs/library/metric_generator).\\n</Note>\\n\\n**Metric Parameters**. Metrics can have optional or required parameters.\\n\\nFor example, the data drift detection algorithm automatically selects a method, but you can override this by specifying your preferred method (Optional).\\n\\n```python\\nreport = Report([\\n   ValueDrift(column=\"target\", method=\"psi\")\\n])\\n```\\n\\nTo calculate the Precision at K for a ranking task, ',\n",
       "  'title': 'Report',\n",
       "  'description': 'How to generate Report.',\n",
       "  'filename': 'docs/library/report.mdx'},\n",
       " {'start': 3000,\n",
       "  'content': '**. To create a custom Report, simply list the Metics one by one. You can combine both dataset-level and column-level Metrics, and combine Presets and Metrics in one Report. When you use a column-level Metric, you must specify the column it refers to.\\n\\n```python\\nreport = Report([\\n    ColumnCount(), \\n    ValueStats(column=\"target\")\\n])\\n\\nmy_eval = report.run(eval_data_1, None)\\nmy_eval\\n#my_eval.json\\n```\\n\\n<Note>\\n  **Generating multiple column-level Metrics**: You can use a helper function to easily generate multiple column-level Metrics for a list of columns. See the page on [Metric Generator](/docs/library/metric_generator).\\n</Note>\\n\\n**Metric Parameters**. Metrics can have optional or required parameters.\\n\\nFor example, the data drift detection algorithm automatically selects a method, but you can override this by specifying your preferred method (Optional).\\n\\n```python\\nreport = Report([\\n   ValueDrift(column=\"target\", method=\"psi\")\\n])\\n```\\n\\nTo calculate the Precision at K for a ranking task, you must always pass the `k` parameter (Required).\\n\\n```python\\nreport = Report([\\n   PrecisionTopK(k=10)\\n])\\n```\\n\\n## Compare results\\n\\nIf you computed multiple snapshots, you can quickly compare the resulting metrics side-by-side in a dataframe:\\n\\n```python\\nfrom evidently import compare\\n\\ncompare_dataframe = compare(my_eval_1, my_eval_2, my_eval_3)\\n```\\n\\n## Group by\\n\\nYou can calculate metrics separately for different groups in your data, using a column with categories to split by. Use the `GroupyBy` metric as shown below.\\n\\n**Example**. This will compute the maximum value of salaries by each label in the \"Department\" column.\\n\\n```python\\nfrom evidently.metrics.group_by import GroupBy\\n\\nreport = Report([\\n    GroupBy(MaxValue(column=\"Salary\"), \"Department\"),\\n])\\nmy_eval = report.run(data, None)\\nmy_eval.dict()\\n```\\n\\nNote: you cannot use auto-generated Test conditions when you use GroupBy.\\n\\n## What\\'s next?\\n\\nYou can also add conditions to Metrics: check the [Tests guide](/docs/library/tests).',\n",
       "  'title': 'Report',\n",
       "  'description': 'How to generate Report.',\n",
       "  'filename': 'docs/library/report.mdx'},\n",
       " {'start': 20000,\n",
       "  'content': 'openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Verbosity\")\\n    ])\\n```\\n\\nRun the Report and view the summary results:\\xa0\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\n![](/images/examples/llm_judge_tutorial_verbosity-min.png)\\n\\nYou can also view the dataframe using `eval_dataset.as_dataframe()`\\n\\n<Info>\\n  Don\\'t fully agree with the results? Use these labels as a starting point, edit the decisions where you see fit - now you\\'ve got your golden dataset\\\\! Next, iterate on your judge prompt. You can also try different evaluator LLMs to see which one does the job better. [How to change an LLM](/metrics/customize_llm_judge#change-the-evaluator-llm).\\n</Info>\\n\\n## What\\'s next?\\n\\nThe LLM judge itself is just one part of your overall evaluation framework. You can integrate this evaluator into different workflows, such as testing your LLM outputs after changing a prompt.\\n\\nTo be able to easily run and compare evals, systematically track the results, and interact with your evaluation dataset, you can use the Evidently Cloud platform.\\n\\n### Set up Evidently Cloud\\n\\n<CloudSignup />\\n\\nImport the components to connect with Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace import CloudWorkspace\\n```\\n\\n### Create a Project\\n\\n<CreateProject />\\n\\n### Send your eval\\n\\nSince you already created the eval, you can simply upload it to the Evidently Cloud.\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nYou can then go to the Evidently Cloud, open your Project and explore the Report.\\n\\n![](/images/examples/llm_judge_tutorial_cloud-min.png)\\n\\n<Info>\\n  You can also [create the LLM judges with no-code](/docs/platform/evals_no_code).\\n</Info>\\n\\n# Reference documentation\\n\\nSee this page for complete [documentation on LLM judges](/metrics/customize_llm_judge).',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'start': 0,\n",
       "  'content': 'import CloudSignup from \\'/snippets/cloud_signup.mdx\\';\\nimport CreateProject from \\'/snippets/create_project.mdx\\';\\n\\nIn this tutorial, we\\'ll show how to evaluate text for custom criteria using LLM as the judge, and evaluate the LLM judge itself.\\n\\n<Info>\\n  **This is a local example.** You will run and explore results using the open-source Python library. At the end, we’ll optionally show how to upload results to the Evidently Platform for easy exploration.\\n</Info>\\n\\nWe\\'ll explore two ways to use an LLM as a judge:\\n\\n- **Reference-based**. Compare new responses against a reference. This is useful for regression testing or whenever you have a \"ground truth\" (approved responses) to compare against.\\n- **Open-ended**. Evaluate responses based on custom criteria, which helps evaluate new outputs when there\\'s no reference available.\\n\\nWe will focus on demonstrating **how to create and tune the LLM evaluator**, which you can then apply in different contexts, like regression testing or prompt comparison.\\n\\n<Info>\\n**Prefer videos?** We also have an extended code tutorial where we iteratively improve the prompt for LLM judge with a video walkthrough:  https://www.youtube.com/watch?v=kP_aaFnXLmY\\n</Info>\\n\\n## Tutorial scope\\n\\nHere\\'s what we\\'ll do:\\n\\n- **Create an evaluation dataset**. Create a toy Q&A dataset.\\n- **Create and run an LLM as a judge**. Design an LLM evaluator prompt.\\n- **Evaluate the judge**. Compare the LLM judge\\'s evaluations with manual labels.\\n\\nWe\\'ll start with the reference-based evaluator that determines whether a new response is correct (it\\'s more complex since it requires passing two columns to the prompt). Then, we\\'ll create a simpler judge focused on verbosity.\\n\\nTo complete the tutorial, you will need:\\n\\n- Basic Python knowledge.\\n- An OpenAI API key to use for the LLM evaluator.\\n\\nWe recommend running this tutorial in Jupyter Notebook or Google Colab to render rich HTML objects with summary results directly in a notebook cell.\\n\\n<Info>\\n  Run a sample notebook: [Jupyter ',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'start': 0,\n",
       "  'content': \"In this tutorial, you will learn how to perform regression testing for LLM outputs.\\n\\nYou can compare new and old responses after changing a prompt, model, or anything else in your system. By re-running the same inputs with new parameters, you can spot any significant changes. This helps you push updates with confidence or identify issues to fix.\\n\\n<Info>\\n  **This example uses Evidently Cloud.** You'll run evals in Python and upload them. You can also skip the upload and view Reports locally. For self-hosted, replace `CloudWorkspace` with `Workspace`.\\n</Info>\\n\\n# Tutorial scope\\n\\nHere's what we'll do:\\n\\n* **Create a toy dataset**. Build a small Q&A dataset with answers and reference responses.\\n\\n* **Get new answers**. Imitate generating new answers to the same question.\\n\\n* **Create and run a Report with Tests**. Compare the answers using LLM-as-a-judge to evaluate length, correctness and style consistency.\\n\\n* **Build a monitoring Dashboard**. Get plots to track the results of Tests over time.\\n\\n<Note>\\n  To simplify things, we won't create an actual LLM app, but will simulate generating new outputs.\\n</Note>\\n\\nTo complete the tutorial, you will need:\\n\\n* Basic Python knowledge.\\xa0\\n\\n* An OpenAI API key to use for the LLM evaluator.\\n\\n* An Evidently Cloud account to track test results. If not yet, [sign up](https://www.evidentlyai.com/register) for a free account.\\n\\n<Info>\\n  You can see all the code in [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb) or click to [open in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb).\\n</Info>\\n\\n## 1. Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently[llm] \\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nfrom evidently.future.datasets import Dataset\\nfrom evidently.future.datasets import DataDefinition\\nfrom evidently.futur\",\n",
       "  'title': 'LLM regression testing',\n",
       "  'description': 'How to run regression testing for LLM outputs.',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'start': 16000,\n",
       "  'content': 'view the scored dataset in Python. This will show a DataFrame with newly added scores and explanations.\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_judge_tutorial_judge_scored_data-min.png)\\n\\n<Info>\\n  **Note**: your explanations will vary since LLMs are non-deterministic.\\n</Info>\\n\\nIf you want, you can also add the column that will help you easily sort and find all error where the LLM-judged label is different from the ground truth label.\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    ExactMatch(columns=[\"label\", \"Correctness\"], alias=\"Judge_match\")])\\n```\\n\\n**Get a Report.** Summarize the result by generating an Evidently Report.\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\nThis will render an HTML report in the notebook cell. You can use other [export options](/docs/library/output_formats), like `as_dict()` for a Python dictionary output.\\n\\n![](/images/examples/llm_judge_tutorial_report-min.png)\\n\\nSince we already performed exact matching, you can see the crude accuracy of our judge. However, accuracy is not always the best metric. In this case, we might be more interested in recall: we want to make sure that the judge does not miss any \"incorrect\" answers .\\n\\n## 4. Evaluate the LLM Eval quality\\n\\nThis part is a bit meta: we\\'re going to evaluate the quality of our LLM evaluator itself\\\\! We can treat it as a simple **binary classification** problem.\\n\\n**Data definition**. To evaluate the classification quality, we need to map the structure of the dataset accordingly first. The column with the manual label is the \"target\", and the LLM-judge response is the \"prediction\":\\n\\n```python\\ndf=eval_dataset.as_dataframe()\\n\\ndefinition_2 = DataDefinition(\\n    classification=[BinaryClassification(\\n        target=\"label\",\\n        prediction_labels=\"Correctness\",\\n        pos_label = \"incorrect\")],\\n    categorical_columns=[\"label\", \"Correctness\"])\\n\\nclass_dataset = Dataset.from_pandas(\\n    pd.DataFra',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'start': 3000,\n",
       "  'content': ' Inputs, context, and outputs (for RAG evaluation)\\n</Info>\\n\\n<Info>\\n  **Collecting live data**. You can also trace inputs and outputs from your LLM app and download the dataset from traces. See the [Tracing Quickstart](/quickstart_tracing)\\n</Info>\\n\\n## 3. Run evaluations\\n\\nWe\\'ll evaluate the answers for:\\n\\n- **Sentiment:** from -1 (negative) to 1 (positive)\\n- **Text length:** character count\\n- **Denials:** refusals to answer. This uses an LLM-as-a-judge with built-in prompt.\\n\\nEach evaluation is a `descriptor`. It adds a new score or label to each row in your dataset.\\n\\nFor LLM-as-a-judge, we\\'ll use OpenAI GPT-4o mini. Set OpenAI key as an environment variable:\\n\\n```python\\n## import os\\n## os.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```\\n\\n<Info>\\n  If you don\\'t have an OpenAI key, you can use a keyword-based check `IncludesWords` instead.\\n</Info>\\n\\nTo run evals, pass the dataset and specify the list of descriptors to add:\\n\\n```python\\neval_dataset = Dataset.from_pandas(\\n    eval_df,\\n    data_definition=DataDefinition(),\\n    descriptors=[\\n        Sentiment(\"answer\", alias=\"Sentiment\"),\\n        TextLength(\"answer\", alias=\"Length\"),\\n        DeclineLLMEval(\"answer\", alias=\"Denials\")]) \\n\\n# Or IncludesWords(\"answer\", words_list=[\\'sorry\\', \\'apologize\\'], alias=\"Denials\")\\n```\\n\\n**Congratulations\\\\!** You\\'ve just run your first eval. Preview the results locally in pandas:\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_quickstart_preview.png)\\n\\n<Info>\\n  **What other evals are there?** Browse all available descriptors including deterministic checks, semantic similarity, and LLM judges in the [descriptor list](/metrics/all_descriptors).\\n</Info>\\n\\n## 4.  Create a Report\\n\\n**Create and run a Report**. It will summarize the evaluation results. \\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\n```\\n\\n**Local preview**. In a Python environment like Jupyter notebook or Colab, run:\\n\\n```python\\nmy_eval\\n```\\n\\nThis will render the Report directly in t',\n",
       "  'title': 'LLM Evaluation',\n",
       "  'description': 'Evaluate text outputs in under 5 minutes',\n",
       "  'filename': 'quickstart_llm.mdx'},\n",
       " {'start': 1000,\n",
       "  'content': 'n.\\n\\n<Info>\\n**Prefer videos?** We also have an extended code tutorial where we iteratively improve the prompt for LLM judge with a video walkthrough:  https://www.youtube.com/watch?v=kP_aaFnXLmY\\n</Info>\\n\\n## Tutorial scope\\n\\nHere\\'s what we\\'ll do:\\n\\n- **Create an evaluation dataset**. Create a toy Q&A dataset.\\n- **Create and run an LLM as a judge**. Design an LLM evaluator prompt.\\n- **Evaluate the judge**. Compare the LLM judge\\'s evaluations with manual labels.\\n\\nWe\\'ll start with the reference-based evaluator that determines whether a new response is correct (it\\'s more complex since it requires passing two columns to the prompt). Then, we\\'ll create a simpler judge focused on verbosity.\\n\\nTo complete the tutorial, you will need:\\n\\n- Basic Python knowledge.\\n- An OpenAI API key to use for the LLM evaluator.\\n\\nWe recommend running this tutorial in Jupyter Notebook or Google Colab to render rich HTML objects with summary results directly in a notebook cell.\\n\\n<Info>\\n  Run a sample notebook: [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb) or [open it in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb).\\n</Info>\\n\\n## 1.  Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently\\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\n\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently import BinaryClassification\\nfrom evidently.descriptors import *\\nfrom evidently.presets import TextEvals, ValueStats, ClassificationPreset\\nfrom evidently.metrics import *\\n\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n```\\n\\nPass your OpenAI key as an environment variable:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n```\\n\\n<Info>\\n**Using other evaluator LLMs**. Check the [LLM judge docs](/metric',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'start': 8000,\n",
       "  'content': 'n LLM judge templates.\\n\\n<Accordion title=\"Custom LLM judge\" description=\"How to create a custom LLM evaluator\" icon=\"sparkles\">\\n  Let\\'s classify user questions as \"appropriate\" or \"inappropriate\" for an educational tool.\\n\\n  ```python\\n  # Define the evaluation criteria\\n  appropriate_scope = BinaryClassificationPromptTemplate(\\n      criteria=\"\"\"An appropriate question is any educational query related to\\n      academic subjects, general school-level world knowledge, or skills.\\n      An inappropriate question is anything offensive, irrelevant, or out of\\n      scope.\"\"\",\\n      target_category=\"APPROPRIATE\",\\n      non_target_category=\"INAPPROPRIATE\",\\n      include_reasoning=True,\\n  )\\n  \\n  # Apply evaluation\\n  llm_evals = Dataset.from_pandas(\\n      eval_df,\\n      data_definition=DataDefinition(),\\n      descriptors=[\\n          LLMEval(\"question\", template=appropriate_scope,\\n                  provider=\"openai\", model=\"gpt-4o-mini\",\\n                  alias=\"Question topic\")\\n      ]\\n  )\\n  \\n  # Run and upload report\\n  report = Report([\\n      TextEvals()\\n  ])\\n  \\n  my_eval = report.run(llm_evals, None)\\n  ws.add_run(project.id, my_eval, include_data=True)\\n  \\n  # Uncomment to replace ws.add_run for a local preview \\n  # my_eval\\n  ```\\n\\n  You can implement any criteria this way, and plug in different LLM models.\\n</Accordion>\\n\\n![](/images/examples/llm_quickstart_descriptor_custom_llm_judge-min.png)\\n\\n## What\\'s next?\\n\\nRead more on how you can configure [LLM judges for custom criteria or using other LLMs](/metrics/customize_llm_judge).\\n\\nWe also have lots of other examples\\\\! [Explore tutorials](/metrics/introduction).',\n",
       "  'title': 'LLM Evaluation',\n",
       "  'description': 'Evaluate text outputs in under 5 minutes',\n",
       "  'filename': 'quickstart_llm.mdx'},\n",
       " {'start': 5000,\n",
       "  'content': 'API. Here is how generating a Report with data summary preset for a single dataset works now:\\n\\n```python\\neval_data = Dataset.from_pandas(\\n    pd.DataFrame(source_df),\\n    data_definition=DataDefinition()\\n)\\n\\nreport = Report([\\n    DataSummaryPreset()\\n])\\n\\nmy_eval = report.run(eval_data, None)\\n```\\n\\nKey changes:\\n\\n- The Report object now defines the configuration (e.g., metrics to include).\\n- Running a Report returns a separate result object.\\n\\n<Card title=\"Reports\" href=\"/docs/library/report\">\\n  How to generate Reports.\\n</Card>\\n\\nAdditional improvement: you can also now use \"Group by\" to compute metrics for specific segments.\\n\\n### Test Suites joined with Reports\\n\\nMost importantly,  Reports and Tests are now unified. Previously, these were separate:\\n\\n- Reports provided an overview of metrics (e.g., distribution summaries, statistics).\\n- Tests verify pass/fail conditions (e.g., check for missing data or LLM quality thresholds).\\n\\nNow, the Test Suite mode is an optional extension of a Report. If you choose to enable Tests, their results appear as a separate tab in the same HTML file. This eliminated duplication and the need to switch between separate files or Reports.\\n\\nFor example, here is how you add a Test on max length that will appear in the same Report as all data / column statistics.\\n\\n```python\\nreport = Report([\\n     DataSummaryPreset(),\\n     MaxValue(column=\"Length\", tests=[lt(100)]),\\n])\\n```\\n\\nYou can still use auto-generated Test conditions based on your reference dataset or define your own expectations.\\n\\n<Card title=\"Tests\" href=\"/docs/library/tests\">\\n  How to add Tests with conditions.\\n</Card>\\n\\n### Metric redesign\\n\\nThe Metric object has been simplified:\\n\\n- Metrics now produce a single computation result with a fixed structure.\\n- Some visualization types can be specified directly as parameters to the Metric.\\n\\nThis redesign significantly improves JSON result parsing and UI integration, since each Metric has a single or two results only.\\n\\nYou can check the list of new Me',\n",
       "  'title': 'Migration Guide',\n",
       "  'description': 'How to migrate to the new Evidently version?',\n",
       "  'filename': 'faq/migration.mdx'},\n",
       " {'start': 17000,\n",
       "  'content': '\\nSince we already performed exact matching, you can see the crude accuracy of our judge. However, accuracy is not always the best metric. In this case, we might be more interested in recall: we want to make sure that the judge does not miss any \"incorrect\" answers .\\n\\n## 4. Evaluate the LLM Eval quality\\n\\nThis part is a bit meta: we\\'re going to evaluate the quality of our LLM evaluator itself\\\\! We can treat it as a simple **binary classification** problem.\\n\\n**Data definition**. To evaluate the classification quality, we need to map the structure of the dataset accordingly first. The column with the manual label is the \"target\", and the LLM-judge response is the \"prediction\":\\n\\n```python\\ndf=eval_dataset.as_dataframe()\\n\\ndefinition_2 = DataDefinition(\\n    classification=[BinaryClassification(\\n        target=\"label\",\\n        prediction_labels=\"Correctness\",\\n        pos_label = \"incorrect\")],\\n    categorical_columns=[\"label\", \"Correctness\"])\\n\\nclass_dataset = Dataset.from_pandas(\\n    pd.DataFrame(df),\\n    data_definition=definition_2)\\n```\\n\\n<Info>\\n  `Pos_label` refers to the class that is treated as the target (\"what we want to predict better\") for metrics like precision, recall, F1-score.\\n</Info>\\n\\n**Get a Report**. Let\\'s use a`ClassificationPreset()` that combines several classification metrics:\\n\\n```python\\nreport = Report([\\n    ClassificationPreset()\\n])\\n\\nmy_eval = report.run(class_dataset, None)\\nmy_eval\\n\\n# or my_eval.as_dict()\\n```\\n\\nWe can now get a well-rounded evaluation and explore the confusion matrix. We have one type of error each: overall the results are pretty good\\\\! You can also refine the prompt to try to improve them.\\n\\n![](/images/examples/llm_judge_tutorial_conf_matrix-min.png)\\n\\n## 5. Verbosity evaluator\\n\\nNext, let’s create a simpler verbosity judge. It will check whether the responses are concise and to the point. This only requires evaluating one output column: such checks are perfect for production evaluations where you don’t have a reference answer.\\n\\nHere\\'s ho',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'start': 4000,\n",
       "  'content': 'e(df),\\n    data_definition=DataDefinition(\\n        text_columns=[\"question\", \"answer\"]),\\n    descriptors=[\\n        Sentiment(\"answer\", alias=\"Sentiment\"),\\n        TextLength(\"answer\", alias=\"Length\"),\\n        IncludesWords(\"answer\", words_list=[\\'sorry\\', \\'apologize\\'], alias=\"Denials\"),\\n    ]\\n)\\n```\\n\\n**2. Aggregate results or run conditional checks**. Use these descriptors like any other dataset column when creating a Report. For example, here is how you summarize all descriptors and check that the text length is under 100 symbols.\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n    MaxValue(column=\"Length\", tests=[lt(100)]),\\n])\\n```\\n\\nThis decoupling means you can reuse descriptor outputs for multiple tests or aggregations without recomputation. It’s especially useful for LLM evaluations.\\n\\n<Card title=\"Descriptors\" href=\"/docs/library/descriptors\">\\n  Docs on adding descriptors.\\n</Card>\\n\\n### New Reports API\\n\\nAs you may have noticed in the example above, we made the changes to the core Report API. Here is how generating a Report with data summary preset for a single dataset works now:\\n\\n```python\\neval_data = Dataset.from_pandas(\\n    pd.DataFrame(source_df),\\n    data_definition=DataDefinition()\\n)\\n\\nreport = Report([\\n    DataSummaryPreset()\\n])\\n\\nmy_eval = report.run(eval_data, None)\\n```\\n\\nKey changes:\\n\\n- The Report object now defines the configuration (e.g., metrics to include).\\n- Running a Report returns a separate result object.\\n\\n<Card title=\"Reports\" href=\"/docs/library/report\">\\n  How to generate Reports.\\n</Card>\\n\\nAdditional improvement: you can also now use \"Group by\" to compute metrics for specific segments.\\n\\n### Test Suites joined with Reports\\n\\nMost importantly,  Reports and Tests are now unified. Previously, these were separate:\\n\\n- Reports provided an overview of metrics (e.g., distribution summaries, statistics).\\n- Tests verify pass/fail conditions (e.g., check for missing data or LLM quality thresholds).\\n\\nNow, the Test Suite mode is an optional extension of a Report. If ',\n",
       "  'title': 'Migration Guide',\n",
       "  'description': 'How to migrate to the new Evidently version?',\n",
       "  'filename': 'faq/migration.mdx'},\n",
       " {'start': 2000,\n",
       "  'content': 'notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb) or [open it in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb).\\n</Info>\\n\\n## 1.  Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently\\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\n\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently import BinaryClassification\\nfrom evidently.descriptors import *\\nfrom evidently.presets import TextEvals, ValueStats, ClassificationPreset\\nfrom evidently.metrics import *\\n\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n```\\n\\nPass your OpenAI key as an environment variable:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n```\\n\\n<Info>\\n**Using other evaluator LLMs**. Check the [LLM judge docs](/metrics/customize_llm_judge#change-the-evaluator-llm) to see how you can select a different evaluator LLM. \\n</Info>\\n\\n## 2.  Create the Dataset\\n\\nFirst, we\\'ll create a toy Q&A dataset with customer support question that includes:\\n\\n- **Questions**. The inputs sent to the LLM app.\\n- **Target responses**. The approved responses you consider accurate.\\n- **New responses**. Imitated new responses from the system.\\n- **Manual labels with explanation**. Labels that say if response is correct or not.\\n\\nWhy add the labels? It\\'s a good idea to be the judge yourself before you write a prompt. This helps:\\n\\n- Formulate better criteria. You discover nuances that help you write a better prompt.\\n- Get the \"ground truth\". You can use it to evaluate the quality of the LLM judge.\\n\\nUltimately, an LLM judge is a small ML system, and it needs its own evals\\\\!\\n\\n**Generate the dataframe**. Here\\'s how you can create this dataset in one go:\\n\\n<Accordion title=\"Toy data to run the example\" defaultOpen={false}>\\n  ```python\\n  ',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'examples/LLM_judge.mdx'}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.search(\n",
    "    'How can I build an eval report with llm as a judge?',\n",
    "    num_results=15\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90948b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.search(\n",
    "    'How can I build an eval report with llm as a judge?',\n",
    "    num_results=15\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e30bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    return index.search(\n",
    "        query=query,\n",
    "        num_results=15\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5cab87e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You're an assistant that helps with the documentation.\n",
    "Answer the QUESTION based on the CONTEXT from the search engine of our documentation.\n",
    "\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "When answering the question, provide the reference to the file with the source.\n",
    "Use the filename field for that. The repo url is: https://github.com/evidentlyai/docs/\n",
    "Include code examples when relevant. \n",
    "If the question is discussed in multiple documents, cite all of them.\n",
    "\n",
    "Don't use markdown or any formatting in the output.\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<CONTEXT>\n",
    "{context}\n",
    "</CONTEXT>\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def build_prompt(question, search_results):\n",
    "    context = json.dumps(search_results)\n",
    "\n",
    "    prompt = prompt_template.format(\n",
    "        question=question,\n",
    "        context=context\n",
    "    ).strip()\n",
    "    \n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c9acaf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(user_prompt, instructions=None, model=\"gpt-4o-mini\"):\n",
    "    messages = []\n",
    "\n",
    "    if instructions:\n",
    "        messages.append({\n",
    "            \"role\": \"system\",\n",
    "            \"content\": instructions\n",
    "        })\n",
    "\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_prompt\n",
    "    })\n",
    "\n",
    "    response = client.responses.create(\n",
    "        model=model,\n",
    "        input=messages\n",
    "    )\n",
    "\n",
    "    return response.output_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "832fb85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    response = llm(prompt)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "35f24ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To build an evaluation report with an LLM (Large Language Model) as a judge, you can follow these steps:\n",
      "\n",
      "### 1. **Set Up Your Environment**\n",
      "Make sure you have the necessary packages installed. You'll need the `evidently` package, which can be installed using:\n",
      "\n",
      "```bash\n",
      "pip install evidently[llm]\n",
      "```\n",
      "\n",
      "### 2. **Import Required Libraries**\n",
      "Import the necessary libraries in your Python script or Jupyter Notebook:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import os\n",
      "from evidently import Dataset, DataDefinition, Report\n",
      "from evidently.llm.templates import BinaryClassificationPromptTemplate\n",
      "from evidently.presets import TextEvals\n",
      "```\n",
      "\n",
      "### 3. **Set Your OpenAI API Key**\n",
      "Set your OpenAI API key as an environment variable:\n",
      "\n",
      "```python\n",
      "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\n",
      "```\n",
      "\n",
      "### 4. **Create Your Evaluation Dataset**\n",
      "You should create a dataset suitable for evaluation, which includes the inputs and expected outputs. Here is an example of how to create a toy Q&A dataset:\n",
      "\n",
      "```python\n",
      "data = {\n",
      "    'question': [\"What is the capital of France?\", \"What is the largest mammal?\"],\n",
      "    'target_response': [\"Paris\", \"Blue whale\"],\n",
      "    'new_response': [\"Paris\", \"Whale\"],\n",
      "    'manual_label': [\"Correct\", \"Incorrect\"]\n",
      "}\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Define dataset\n",
      "eval_dataset = Dataset.from_pandas(\n",
      "    df,\n",
      "    data_definition=DataDefinition(text_columns=[\"question\", \"new_response\"]),\n",
      "    descriptors=[\n",
      "        # Add any relevant descriptors you need.\n",
      "    ]\n",
      ")\n",
      "```\n",
      "\n",
      "### 5. **Configure the LLM as a Judge**\n",
      "Define your LLM prompt criteria using a template. For instance, if you're classifying responses based on correctness:\n",
      "\n",
      "```python\n",
      "criteria = BinaryClassificationPromptTemplate(\n",
      "    criteria=\"An appropriate response is factual and accurate.\",\n",
      "    target_category=\"CORRECT\",\n",
      "    non_target_category=\"INCORRECT\",\n",
      "    include_reasoning=True,\n",
      ")\n",
      "\n",
      "# Evaluate using the LLM\n",
      "llm_evals = Dataset.from_pandas(\n",
      "    df,\n",
      "    data_definition=DataDefinition(),\n",
      "    descriptors=[\n",
      "        LLMEval(\"new_response\", template=criteria, provider=\"openai\")\n",
      "    ]\n",
      ")\n",
      "```\n",
      "\n",
      "### 6. **Run the Evaluation and Generate a Report**\n",
      "Now, you can generate the evaluation report:\n",
      "\n",
      "```python\n",
      "report = Report([TextEvals()])\n",
      "my_eval = report.run(llm_evals, None)\n",
      "\n",
      "# View the report\n",
      "my_eval.as_dataframe()  # To see the DataFrame with scores\n",
      "```\n",
      "\n",
      "### 7. **Analyze the Results**\n",
      "Finally, analyze the results of your evaluation. The report will include metrics that summarize the performance of your model as judged by the LLM.\n",
      "\n",
      "### Conclusion\n",
      "By following these steps, you can effectively build an evaluation report with an LLM as a judge, allowing you to assess the quality of responses and make improvements based on the insights gathered.\n"
     ]
    }
   ],
   "source": [
    "result = rag('How can I build an eval report with llm as a judge?')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc24ac1",
   "metadata": {},
   "source": [
    "# Intelligent Chunking\n",
    "https://maven.com/alexey-grigorev/from-rag-to-agents/1/syllabus/modules/b9fa7b?item=egxqvfwaq0m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108183d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00c4320",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-bootcamp (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

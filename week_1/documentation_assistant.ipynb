{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c438df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documentation Assistant with GitHub RAG\n",
    "\n",
    "## Import Libraries\n",
    "\n",
    "import re\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "import json\n",
    "from minsearch import Index, VectorSearch\n",
    "import pickle \n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27e114d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GitHub Repository Reader\n",
    "\n",
    "# Utility class to download and parse files from GitHub repositories\n",
    "\n",
    "import io\n",
    "from typing import Iterable, Callable\n",
    "import zipfile\n",
    "import traceback\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RawRepositoryFile:\n",
    "    filename: str\n",
    "    content: str\n",
    "\n",
    "\n",
    "class GithubRepositoryDataReader:\n",
    "    \"\"\"\n",
    "    Downloads and parses markdown and code files from a GitHub repository.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                repo_owner: str,\n",
    "                repo_name: str,\n",
    "                allowed_extensions: Iterable[str] | None = None,\n",
    "                filename_filter: Callable[[str], bool] | None = None\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initialize the GitHub repository data reader.\n",
    "        \n",
    "        Args:\n",
    "            repo_owner: The owner/organization of the GitHub repository\n",
    "            repo_name: The name of the GitHub repository\n",
    "            allowed_extensions: Optional set of file extensions to include\n",
    "                    (e.g., {\"md\", \"py\"}). If not provided, all file types are included\n",
    "            filename_filter: Optional callable to filter files by their path\n",
    "        \"\"\"\n",
    "        prefix = \"https://codeload.github.com\"\n",
    "        self.url = (\n",
    "            f\"{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main\"\n",
    "        )\n",
    "\n",
    "        if allowed_extensions is not None:\n",
    "            self.allowed_extensions = {ext.lower() for ext in allowed_extensions}\n",
    "\n",
    "        if filename_filter is None:\n",
    "            self.filename_filter = lambda filepath: True\n",
    "        else:\n",
    "            self.filename_filter = filename_filter\n",
    "\n",
    "    def read(self) -> list[RawRepositoryFile]:\n",
    "        \"\"\"\n",
    "        Download and extract files from the GitHub repository.\n",
    "        \n",
    "        Returns:\n",
    "            List of RawRepositoryFile objects for each processed file\n",
    "            \n",
    "        Raises:\n",
    "            Exception: If the repository download fails\n",
    "        \"\"\"\n",
    "        resp = requests.get(self.url)\n",
    "        if resp.status_code != 200:\n",
    "            raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "        zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "        repository_data = self._extract_files(zf)\n",
    "        zf.close()\n",
    "\n",
    "        return repository_data\n",
    "\n",
    "    def _extract_files(self, zf: zipfile.ZipFile) -> list[RawRepositoryFile]:\n",
    "        \"\"\"\n",
    "        Extract and process files from the zip archive.\n",
    "        \n",
    "        Args:\n",
    "            zf: ZipFile object containing the repository data\n",
    "\n",
    "        Returns:\n",
    "            List of RawRepositoryFile objects for each processed file\n",
    "        \"\"\"\n",
    "        data = []\n",
    "\n",
    "        for file_info in zf.infolist():\n",
    "            filepath = self._normalize_filepath(file_info.filename)\n",
    "\n",
    "            if self._should_skip_file(filepath):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with zf.open(file_info) as f_in:\n",
    "                    content = f_in.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "                    if content is not None:\n",
    "                        content = content.strip()\n",
    "\n",
    "                    file = RawRepositoryFile(\n",
    "                        filename=filepath,\n",
    "                        content=content\n",
    "                    )\n",
    "                    data.append(file)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_info.filename}: {e}\")\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _should_skip_file(self, filepath: str) -> bool:\n",
    "        \"\"\"\n",
    "        Determine whether a file should be skipped during processing.\n",
    "        \n",
    "        Args:\n",
    "            filepath: The file path to check\n",
    "            \n",
    "        Returns:\n",
    "            True if the file should be skipped, False otherwise\n",
    "        \"\"\"\n",
    "        filepath = filepath.lower()\n",
    "\n",
    "        # directory\n",
    "        if filepath.endswith(\"/\"):\n",
    "            return True\n",
    "\n",
    "        # hidden file\n",
    "        filename = filepath.split(\"/\")[-1]\n",
    "        if filename.startswith(\".\"):\n",
    "            return True\n",
    "\n",
    "        if self.allowed_extensions:\n",
    "            ext = self._get_extension(filepath)\n",
    "            if ext not in self.allowed_extensions:\n",
    "                return True\n",
    "\n",
    "        if not self.filename_filter(filepath):\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _get_extension(self, filepath: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract the file extension from a filepath.\n",
    "        \n",
    "        Args:\n",
    "            filepath: The file path to extract extension from\n",
    "            \n",
    "        Returns:\n",
    "            The file extension (without dot) or empty string if no extension\n",
    "        \"\"\"\n",
    "        filename = filepath.lower().split(\"/\")[-1]\n",
    "        if \".\" in filename:\n",
    "            return filename.rsplit(\".\", maxsplit=1)[-1]\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    def _normalize_filepath(self, filepath: str) -> str:\n",
    "        \"\"\"\n",
    "        Removes the top-level directory from the file path inside the zip archive.\n",
    "        'repo-main/path/to/file.py' -> 'path/to/file.py'\n",
    "        \n",
    "        Args:\n",
    "            filepath: The original filepath from the zip archive\n",
    "            \n",
    "        Returns:\n",
    "            The normalized filepath with top-level directory removed\n",
    "        \"\"\"\n",
    "        parts = filepath.split(\"/\", maxsplit=1)\n",
    "        if len(parts) > 1:\n",
    "            return parts[1]\n",
    "        else:\n",
    "            return parts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5833c1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download Documentation from Evidently AI\n",
    "\n",
    "def read_github_data():\n",
    "    \"\"\"Download markdown documentation files from evidentlyai/docs\"\"\"\n",
    "    allowed_extensions = {\"md\", \"mdx\"}\n",
    "\n",
    "    repo_owner = 'evidentlyai'\n",
    "    repo_name = 'docs'\n",
    "\n",
    "    reader = GithubRepositoryDataReader(\n",
    "        repo_owner,\n",
    "        repo_name,\n",
    "        allowed_extensions=allowed_extensions\n",
    "    )\n",
    "    \n",
    "    return reader.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c45120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the documentation\n",
    "github_data = read_github_data()\n",
    "print(f\"Downloaded {len(github_data)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b31193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a sample file\n",
    "github_data[40].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c66eb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parse Markdown Frontmatter\n",
    "\n",
    "import frontmatter\n",
    "\n",
    "def parse_data(data_raw: List[RawRepositoryFile]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Extract frontmatter and content from markdown files\"\"\"\n",
    "    data_parsed = []\n",
    "    for f in data_raw:\n",
    "        post = frontmatter.loads(f.content)\n",
    "        data = post.to_dict()\n",
    "        data['filename'] = f.filename\n",
    "        data_parsed.append(data)\n",
    "\n",
    "    return data_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e7b9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Document Chunking Utilities\n",
    "\n",
    "\"\"\"\n",
    "Document chunking utilities for splitting large documents into smaller, overlapping pieces.\n",
    "\n",
    "This module provides functionality to break down documents into chunks using a sliding\n",
    "window approach, which is useful for processing large texts in smaller, manageable pieces\n",
    "while maintaining context through overlapping content.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Any, Dict, Iterable, List\n",
    "\n",
    "\n",
    "def sliding_window(\n",
    "        seq: Iterable[Any],\n",
    "        size: int,\n",
    "        step: int\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create overlapping chunks from a sequence using a sliding window approach.\n",
    "\n",
    "    Args:\n",
    "        seq: The input sequence (string or list) to be chunked.\n",
    "        size (int): The size of each chunk/window.\n",
    "        step (int): The step size between consecutive windows.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each containing:\n",
    "            - 'start': The starting position of the chunk in the original sequence\n",
    "            - 'content': The chunk content\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If size or step are not positive integers.\n",
    "\n",
    "    Example:\n",
    "        >>> sliding_window(\"hello world\", size=5, step=3)\n",
    "        [{'start': 0, 'content': 'hello'}, {'start': 3, 'content': 'lo wo'}]\n",
    "    \"\"\"\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        batch = seq[i:i+size]\n",
    "        result.append({'start': i, 'content': batch})\n",
    "        if i + size > n:\n",
    "            break\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def chunk_documents(\n",
    "        documents: Iterable[Dict[str, str]],\n",
    "        size: int = 2000,\n",
    "        step: int = 1000,\n",
    "        content_field_name: str = 'content'\n",
    ") -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Split a collection of documents into smaller chunks using sliding windows.\n",
    "\n",
    "    Takes documents and breaks their content into overlapping chunks while preserving\n",
    "    all other document metadata (filename, etc.) in each chunk.\n",
    "\n",
    "    Args:\n",
    "        documents: An iterable of document dictionaries. Each document must have a content field.\n",
    "        size (int, optional): The maximum size of each chunk. Defaults to 2000.\n",
    "        step (int, optional): The step size between chunks. Defaults to 1000.\n",
    "        content_field_name (str, optional): The name of the field containing document content.\n",
    "                                          Defaults to 'content'.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of chunk dictionaries. Each chunk contains:\n",
    "            - All original document fields except the content field\n",
    "            - 'start': Starting position of the chunk in original content\n",
    "            - 'content': The chunk content\n",
    "\n",
    "    Example:\n",
    "        >>> documents = [{'content': 'long text...', 'filename': 'doc.txt'}]\n",
    "        >>> chunks = chunk_documents(documents, size=100, step=50)\n",
    "        >>> # Or with custom content field:\n",
    "        >>> documents = [{'text': 'long text...', 'filename': 'doc.txt'}]\n",
    "        >>> chunks = chunk_documents(documents, content_field_name='text')\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for doc in documents:\n",
    "        doc_copy = doc.copy()\n",
    "        doc_content = doc_copy.pop(content_field_name)\n",
    "        chunks = sliding_window(doc_content, size=size, step=step)\n",
    "        for chunk in chunks:\n",
    "            chunk.update(doc_copy)\n",
    "        results.extend(chunks)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d5f0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Index Documents with Optional Chunking\n",
    "\n",
    "\"\"\"\n",
    "Document indexing utilities for creating searchable indexes from document collections.\n",
    "\n",
    "This module provides functionality to index documents using minsearch, with optional\n",
    "chunking support for handling large documents.\n",
    "\"\"\"\n",
    "\n",
    "from minsearch import Index\n",
    "\n",
    "\n",
    "def index_documents(documents, chunk: bool = False, chunking_params=None) -> Index:\n",
    "    \"\"\"\n",
    "    Create a searchable index from a collection of documents.\n",
    "\n",
    "    Args:\n",
    "        documents: A collection of document dictionaries, each containing at least\n",
    "                  'content' and 'filename' fields.\n",
    "        chunk (bool, optional): Whether to chunk documents before indexing.\n",
    "                               Defaults to False.\n",
    "        chunking_params (dict, optional): Parameters for document chunking.\n",
    "                                        Defaults to {'size': 2000, 'step': 1000}.\n",
    "                                        Only used when chunk=True.\n",
    "\n",
    "    Returns:\n",
    "        Index: A fitted minsearch Index object ready for searching.\n",
    "\n",
    "    Example:\n",
    "        >>> docs = [{'content': 'Hello world', 'filename': 'doc1.txt'}]\n",
    "        >>> index = index_documents(docs)\n",
    "        >>> results = index.search('hello')\n",
    "    \"\"\"\n",
    "    if chunk:\n",
    "        if chunking_params is None:\n",
    "            chunking_params = {'size': 2000, 'step': 1000}\n",
    "        documents = chunk_documents(documents, **chunking_params)\n",
    "\n",
    "    index = Index(\n",
    "        text_fields=[\"content\", \"filename\"],\n",
    "    )\n",
    "\n",
    "    index.fit(documents)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b15fb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the downloaded files\n",
    "parsed_data = parse_data(github_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f834ac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a parsed document\n",
    "parsed_data[10]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131332d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Searchable Index with Chunking\n",
    "\n",
    "# Chunk large documents for better retrieval granularity\n",
    "index = index_documents(\n",
    "        parsed_data,\n",
    "        chunk=True,\n",
    "        chunking_params={\"size\": 2000, \"step\": 1000},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fd9f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the search\n",
    "index.search(\n",
    "    'How can I build an eval report with llm as a judge?',\n",
    "    num_results=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e30bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define RAG Helper Functions\n",
    "\n",
    "def search(query):\n",
    "    \"\"\"Search the documentation index\"\"\"\n",
    "    return index.search(\n",
    "        query=query,\n",
    "        num_results=15\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cab87e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configure System Instructions and Prompt\n",
    "\n",
    "instructions = \"\"\"\n",
    "You're an assistant that helps with the documentation.\n",
    "Answer the QUESTION based on the CONTEXT from the search engine of our documentation.\n",
    "\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "When answering the question, provide the reference to the file with the source.\n",
    "Use the filename field for that. The repo url is: https://github.com/evidentlyai/docs/\n",
    "Include code examples when relevant. \n",
    "If the question is discussed in multiple documents, cite all of them.\n",
    "\n",
    "Don't use markdown or any formatting in the output.\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<CONTEXT>\n",
    "{context}\n",
    "</CONTEXT>\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def build_prompt(question, search_results):\n",
    "    \"\"\"Build prompt with question and context\"\"\"\n",
    "    context = json.dumps(search_results)\n",
    "\n",
    "    prompt = prompt_template.format(\n",
    "        question=question,\n",
    "        context=context\n",
    "    ).strip()\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9acaf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(user_prompt, instructions=None, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"Call OpenAI with system instructions\"\"\"\n",
    "    messages = []\n",
    "\n",
    "    if instructions:\n",
    "        messages.append({\n",
    "            \"role\": \"system\",\n",
    "            \"content\": instructions\n",
    "        })\n",
    "\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_prompt\n",
    "    })\n",
    "\n",
    "    response = client.responses.create(\n",
    "        model=model,\n",
    "        input=messages\n",
    "    )\n",
    "\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832fb85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    \"\"\"Complete RAG pipeline for documentation Q&A\"\"\"\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    response = llm(prompt, instructions=instructions)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f24ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test the Documentation Assistant\n",
    "\n",
    "result = rag('How can I build an eval report with llm as a judge?')\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-bootcamp (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
